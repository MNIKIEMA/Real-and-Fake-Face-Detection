{"cells":[{"cell_type":"code","execution_count":29,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-26T22:04:36.549277Z","iopub.status.busy":"2022-12-26T22:04:36.548373Z","iopub.status.idle":"2022-12-26T22:04:36.802696Z","shell.execute_reply":"2022-12-26T22:04:36.801844Z","shell.execute_reply.started":"2022-12-26T22:04:36.549242Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torch.nn as nn\n","import copy\n","import torch\n","from torchvision import transforms\n","from torch.optim import lr_scheduler\n","import os\n","import numpy as np\n","from skimage import io\n","import os\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data preparation"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:04:36.805012Z","iopub.status.busy":"2022-12-26T22:04:36.804543Z","iopub.status.idle":"2022-12-26T22:04:36.816121Z","shell.execute_reply":"2022-12-26T22:04:36.815151Z","shell.execute_reply.started":"2022-12-26T22:04:36.804973Z"},"trusted":true},"outputs":[],"source":["class FaceDataset(Dataset):\n","\n","    def __init__(self, image_dir, transform=None):\n","        \n","        \"\"\"Function to load images into Tensor\n","            Args: \n","                - image_dir : directory of images\n","                - Return : a dictonary with images and labels\n","                \"\"\"\n","        self.image_dir = image_dir\n","        self.image_dict = self.load_image()\n","        self.transform = transform\n","\n","\n","    def __len__(self) :\n","        return len(self.image_dict[\"label\"])\n","\n","\n","    def __getitem__(self, index) :\n","        \n","        \n","        path = torch.from_numpy(io.imread(self.image_dict[\"img_dir\"][index],\n","                                         as_gray=True).astype(np.float32)).unsqueeze(0)\n","        label = self.image_dict[\"label_bin\"][index]\n","\n","        if self.transform:\n","            path = self.transform(path)\n","        \n","        return path, label\n","\n","\n","    def load_image(self) :\n","        img_dict = {\"img_dir\" : [], \"label\" : [], 'label_bin':[]}\n","        for root, dirs, files in os.walk(self.image_dir):\n","            for img in files:\n","                img_dict[\"img_dir\"].append(os.path.join(root, img))\n","                \n","                img_dict[\"label\"].append(img[:4])\n","                if img[:4] != 'real':\n","                    img_dict[\"label_bin\"].append(0.0)\n","                else:\n","                    img_dict[\"label_bin\"].append(1.0)\n","        img_dict[\"label_bin\"] = torch.tensor(img_dict[\"label_bin\"],dtype=torch.float32)\n","        return img_dict"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:04:36.818225Z","iopub.status.busy":"2022-12-26T22:04:36.817824Z","iopub.status.idle":"2022-12-26T22:04:36.830998Z","shell.execute_reply":"2022-12-26T22:04:36.830139Z","shell.execute_reply.started":"2022-12-26T22:04:36.818178Z"},"trusted":true},"outputs":[],"source":["def split_data(datas, train_size=0.8):\n","    \n","    \"\"\" Function to split data in training, valid and testing\n","        Args:   data torch Dataset\n","                train_size the training data size \"\"\"\n","    training_size = int(train_size*len(datas))\n","    train_data, test_data = random_split(datas, [training_size, len(datas)-training_size])\n","    valid_size = int(0.1*len(train_data))\n","    train_data, valid_data = random_split(train_data, [len(train_data)-valid_size,\n","                                                   valid_size])\n","    return train_data, valid_data, test_data"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:04:36.834077Z","iopub.status.busy":"2022-12-26T22:04:36.833738Z","iopub.status.idle":"2022-12-26T22:04:36.866962Z","shell.execute_reply":"2022-12-26T22:04:36.866156Z","shell.execute_reply.started":"2022-12-26T22:04:36.834045Z"},"trusted":true},"outputs":[],"source":["data = FaceDataset(\"/data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-26T22:33:51.297020Z","iopub.status.idle":"2022-12-26T22:33:51.297825Z","shell.execute_reply":"2022-12-26T22:33:51.297576Z","shell.execute_reply.started":"2022-12-26T22:33:51.297551Z"},"trusted":true},"outputs":[],"source":["tf = transforms.Compose([#transforms.Resize((30,30)),\n","    #transforms.Grayscale(num_output_channels=1),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(20),\n","    #transforms.ToTensor()\n","    ])\n","dataset = FaceDataset(\"data\", transform=tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-26T22:33:51.299288Z","iopub.status.idle":"2022-12-26T22:33:51.300046Z","shell.execute_reply":"2022-12-26T22:33:51.299827Z","shell.execute_reply.started":"2022-12-26T22:33:51.299802Z"},"trusted":true},"outputs":[],"source":["train_data, valid_data, test_data = split_data(data)\n","train_data_loader = DataLoader(dataset= train_data,batch_size=10, shuffle=True, \n","                               drop_last=True)\n","valid_data_loader = DataLoader(dataset= valid_data,batch_size=10, shuffle=True, \n","                               drop_last=True)\n","test_data_loader = DataLoader(dataset=test_data, batch_size=10, shuffle=True, \n","                              drop_last=True)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:34:17.222789Z","iopub.status.busy":"2022-12-26T22:34:17.222086Z","iopub.status.idle":"2022-12-26T22:34:17.227503Z","shell.execute_reply":"2022-12-26T22:34:17.226407Z","shell.execute_reply.started":"2022-12-26T22:34:17.222747Z"},"trusted":true},"outputs":[],"source":["data_aug = torch.utils.data.ConcatDataset([train_data, dataset])"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:35:01.662256Z","iopub.status.busy":"2022-12-26T22:35:01.661558Z","iopub.status.idle":"2022-12-26T22:35:01.667890Z","shell.execute_reply":"2022-12-26T22:35:01.666630Z","shell.execute_reply.started":"2022-12-26T22:35:01.662206Z"},"trusted":true},"outputs":[],"source":["train_aug_loader = DataLoader(dataset= aug_train,batch_size=32, shuffle=True, \n","                               drop_last=True)\n","#valid_aug_loader = DataLoader(dataset= aug_valid,batch_size=10, shuffle=True, \n","                               #drop_last=True)\n","#test_aug_loader = DataLoader(dataset=aug_test, batch_size=10, shuffle=True, \n","                              #drop_last=True)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:09:14.100684Z","iopub.status.busy":"2022-12-26T22:09:14.100320Z","iopub.status.idle":"2022-12-26T22:09:14.259229Z","shell.execute_reply":"2022-12-26T22:09:14.258272Z","shell.execute_reply.started":"2022-12-26T22:09:14.100651Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([10, 1, 600, 600])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["next(iter(train_data_loader))[0].shape"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:35:07.022358Z","iopub.status.busy":"2022-12-26T22:35:07.021465Z","iopub.status.idle":"2022-12-26T22:35:07.032160Z","shell.execute_reply":"2022-12-26T22:35:07.030995Z","shell.execute_reply.started":"2022-12-26T22:35:07.022311Z"},"trusted":true},"outputs":[],"source":["class MLPClassif(nn.Module):\n","    \n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(MLPClassif, self).__init__()\n","        self.hidden1 = nn.Linear(input_size, hidden_size)\n","        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n","        self.hidden3 = nn.Linear(hidden_size, hidden_size)\n","        #self.hidden4 = nn.Linear(hidden_size, hidden_size)\n","        self.out_layer = nn.Linear(hidden_size, output_size)\n","        self.relu = nn.ReLU()\n","        self.batchnorm = nn.BatchNorm1d(hidden_size, affine=False)\n","\n","    \n","    def forward(self, x):\n","\n","        #sigmoid = nn.Sigmoid()\n","        dropout = nn.Dropout(p=0.1)\n","        x = self.hidden1(x)\n","        x = self.relu(x)\n","        x = dropout(x)\n","        x = self.batchnorm(x)\n","        x = self.hidden2(x)\n","        x = self.relu(x)\n","        #x = dropout(x)\n","        x = self.batchnorm(x)\n","        x = self.hidden3(x)\n","        x = self.relu(x)\n","        #x = dropout(x)\n","        #x = self.hidden4(x)\n","        #x = self.relu(x)\n","        x = self.batchnorm(x)\n","        #x = dropout(x)\n","        out = self.out_layer(x)\n","        return out"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:35:07.988882Z","iopub.status.busy":"2022-12-26T22:35:07.988187Z","iopub.status.idle":"2022-12-26T22:35:08.001864Z","shell.execute_reply":"2022-12-26T22:35:08.000681Z","shell.execute_reply.started":"2022-12-26T22:35:07.988843Z"},"trusted":true},"outputs":[{"data":{"text/plain":["MLPClassif(\n","  (hidden1): Linear(in_features=900, out_features=100, bias=True)\n","  (hidden2): Linear(in_features=100, out_features=100, bias=True)\n","  (hidden3): Linear(in_features=100, out_features=100, bias=True)\n","  (out_layer): Linear(in_features=100, out_features=1, bias=True)\n","  (relu): ReLU()\n","  (batchnorm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",")"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["model = MLPClassif(30*30, 100, 1)\n","def init_weights(m):\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight.data)\n","        m.bias.data.fill_(0.01)\n","        \n","\n","#torch.manual_seed(0)\n","model.apply(init_weights)"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:35:09.102296Z","iopub.status.busy":"2022-12-26T22:35:09.101962Z","iopub.status.idle":"2022-12-26T22:35:09.116656Z","shell.execute_reply":"2022-12-26T22:35:09.115792Z","shell.execute_reply.started":"2022-12-26T22:35:09.102266Z"},"trusted":true},"outputs":[],"source":["def accuracy_fn(y_true, y_pred):\n","    correct = torch.eq(y_true, y_pred).sum().item()\n","    acc = (correct / len(y_pred)) * 100 \n","    return acc\n","\n","\n","def eval_binary_classifier(model, eval_dataloader, loss_fn):\n","    sigmoid = nn.Sigmoid()\n","    model.eval() \n","    #model.to(device)\n","    transform = transforms.Resize(size = (30,30))\n","    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n","    with torch.no_grad():\n","        # initialize the total and correct number of labels to compute the accuracy\n","        loss, accuracy = 0, 0\n","        # Iterate over the dataset using the dataloader\n","        for images, labels in eval_dataloader:\n","            #images = torch.tensor(images, dtype=float)\n","            images = transform(images)\n","            \n","            #labels.to(device)\n","            images = images.reshape(images.shape[0], -1).to(device)\n","            #images.to(device)\n","            #print(images.shape)\n","            # Get the predicted labels\n","            y_predicted = model(images)\n","\n","            l =loss_fn(y_predicted, labels.unsqueeze(1).to(device))\n","            loss += l.item()\n","            accuracy += accuracy_fn(labels.to(device), torch.round(sigmoid(y_predicted)).squeeze(1))\n","        accuracy = accuracy/len(eval_dataloader)\n","\n","    return loss, accuracy\n","\n","def train_val_binary_classifier(model, train_dataloader, valid_dataloader, num_epochs, \n","                            loss_fn, learning_rate, verbose=True):\n","\n","    # Make a copy of the model (avoid changing the model outside this function)\n","    model_tr = copy.deepcopy(model)\n","    model_tr = model_tr.cuda()\n","    \n","    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n","    model_tr.train()\n","    #optimizer = #torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n","    optimizer =torch.optim.SGD(model_tr.parameters(), lr=learning_rate)\n","    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","    # Initialize a list to record the training loss over epochs\n","    loss_all_epochs = []\n","    valid_loss = []\n","    acuracy = 0\n","    transform = transforms.Resize(size = (30,30))\n","    # Training loop\n","\n","    for epoch in range(num_epochs):\n","        # Initialize the training loss for the current epoch\n","        loss_current_epoch = 0\n","        \n","        # Iterate over batches using the dataloader\n","        for batch_index, (images, labels) in enumerate(train_dataloader):\n","            images = transform(images)\n","            images = images.reshape(images.shape[0], -1)\n","            \n","            \n","            y_pred = model_tr(images.to(device))\n","            #print(f\"size of ypred {y_pred}\")\n","            # print(f\"size of labels {labels.unsqueeze(1).shape}\")\n","            l = loss_fn(y_pred, labels.unsqueeze(1).to(device))\n","\n","            \n","            optimizer.zero_grad()\n","            l.backward()\n","            optimizer.step()\n","            loss_current_epoch +=  l.item()\n","            #accuracy += accuracy_fn(labels, torch.round(sigmoid(y_pred)).squeeze(1))\n","\n","\n","        scheduler.step()\n","        loss_all_epochs.append(loss_current_epoch)\n","        val_loss, accuracy = eval_binary_classifier(model_tr, \n","                                                 valid_dataloader,\n","                                                 loss_fn)\n","        valid_loss.append(val_loss)\n","\n","                                    \n","        if verbose:\n","            print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\"\n","                  f\"\\nTrain loss: {loss_current_epoch:.5f} | \"\n","                  f\"Test loss: {val_loss:.5f} | Test acc: {accuracy:.2f}%\\n\")\n","            \n","\n","            \n","        \n","    return model_tr, loss_all_epochs, valid_loss"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2022-12-26T22:35:37.125161Z","iopub.status.busy":"2022-12-26T22:35:37.124668Z","iopub.status.idle":"2022-12-26T23:17:23.825523Z","shell.execute_reply":"2022-12-26T23:17:23.824365Z","shell.execute_reply.started":"2022-12-26T22:35:37.125122Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","\n","Epoch [1/50]\n","Train loss: 66.95740 | Test loss: 26.83195 | Test acc: 55.62%\n","\n","\n","Epoch [2/50]\n","Train loss: 75.09399 | Test loss: 24.53105 | Test acc: 50.31%\n","\n","\n","Epoch [3/50]\n","Train loss: 63.31080 | Test loss: 24.72954 | Test acc: 50.62%\n","\n","\n","Epoch [4/50]\n","Train loss: 62.07545 | Test loss: 21.45687 | Test acc: 58.12%\n","\n","\n","Epoch [5/50]\n","Train loss: 61.24953 | Test loss: 20.94338 | Test acc: 61.88%\n","\n","\n","Epoch [6/50]\n","Train loss: 60.89502 | Test loss: 21.23224 | Test acc: 60.31%\n","\n","\n","Epoch [7/50]\n","Train loss: 59.99446 | Test loss: 21.58044 | Test acc: 58.12%\n","\n","\n","Epoch [8/50]\n","Train loss: 59.06878 | Test loss: 20.34067 | Test acc: 67.19%\n","\n","\n","Epoch [9/50]\n","Train loss: 58.31242 | Test loss: 20.88385 | Test acc: 61.56%\n","\n","\n","Epoch [10/50]\n","Train loss: 57.38644 | Test loss: 20.12618 | Test acc: 65.62%\n","\n","\n","Epoch [11/50]\n","Train loss: 57.24799 | Test loss: 19.90862 | Test acc: 65.00%\n","\n","\n","Epoch [12/50]\n","Train loss: 55.62573 | Test loss: 20.61035 | Test acc: 63.12%\n","\n","\n","Epoch [13/50]\n","Train loss: 55.51096 | Test loss: 20.71474 | Test acc: 61.25%\n","\n","\n","Epoch [14/50]\n","Train loss: 54.02048 | Test loss: 19.51279 | Test acc: 65.00%\n","\n","\n","Epoch [15/50]\n","Train loss: 53.96213 | Test loss: 19.79968 | Test acc: 63.12%\n","\n","\n","Epoch [16/50]\n","Train loss: 53.17397 | Test loss: 19.12594 | Test acc: 64.06%\n","\n","\n","Epoch [17/50]\n","Train loss: 52.49639 | Test loss: 19.35071 | Test acc: 65.31%\n","\n","\n","Epoch [18/50]\n","Train loss: 51.47392 | Test loss: 18.41596 | Test acc: 70.00%\n","\n","\n","Epoch [19/50]\n","Train loss: 50.81787 | Test loss: 18.56578 | Test acc: 71.56%\n","\n","\n","Epoch [20/50]\n","Train loss: 49.55249 | Test loss: 18.30056 | Test acc: 70.00%\n","\n","\n","Epoch [21/50]\n","Train loss: 49.46364 | Test loss: 18.32387 | Test acc: 68.44%\n","\n","\n","Epoch [22/50]\n","Train loss: 48.66322 | Test loss: 17.87966 | Test acc: 70.94%\n","\n","\n","Epoch [23/50]\n","Train loss: 48.22241 | Test loss: 17.36230 | Test acc: 71.56%\n","\n","\n","Epoch [24/50]\n","Train loss: 48.16022 | Test loss: 19.00625 | Test acc: 66.56%\n","\n","\n","Epoch [25/50]\n","Train loss: 46.47081 | Test loss: 19.47193 | Test acc: 65.00%\n","\n","\n","Epoch [26/50]\n","Train loss: 46.19386 | Test loss: 17.71110 | Test acc: 69.38%\n","\n","\n","Epoch [27/50]\n","Train loss: 45.69558 | Test loss: 17.05414 | Test acc: 71.56%\n","\n","\n","Epoch [28/50]\n","Train loss: 45.90696 | Test loss: 17.56833 | Test acc: 71.25%\n","\n","\n","Epoch [29/50]\n","Train loss: 44.34891 | Test loss: 16.92856 | Test acc: 75.31%\n","\n","\n","Epoch [30/50]\n","Train loss: 44.60678 | Test loss: 17.96811 | Test acc: 74.69%\n","\n","\n","Epoch [31/50]\n","Train loss: 44.55883 | Test loss: 16.58788 | Test acc: 75.00%\n","\n","\n","Epoch [32/50]\n","Train loss: 44.13130 | Test loss: 17.14450 | Test acc: 72.50%\n","\n","\n","Epoch [33/50]\n","Train loss: 43.90232 | Test loss: 16.52495 | Test acc: 73.75%\n","\n","\n","Epoch [34/50]\n","Train loss: 43.51013 | Test loss: 16.74261 | Test acc: 75.00%\n","\n","\n","Epoch [35/50]\n","Train loss: 43.05347 | Test loss: 15.98796 | Test acc: 74.69%\n","\n","\n","Epoch [36/50]\n","Train loss: 42.90061 | Test loss: 17.18326 | Test acc: 72.81%\n","\n","\n","Epoch [37/50]\n","Train loss: 42.63687 | Test loss: 16.39547 | Test acc: 74.38%\n","\n","\n","Epoch [38/50]\n","Train loss: 42.19184 | Test loss: 16.46310 | Test acc: 73.12%\n","\n","\n","Epoch [39/50]\n","Train loss: 41.79951 | Test loss: 16.57653 | Test acc: 73.12%\n","\n","\n","Epoch [40/50]\n","Train loss: 42.01983 | Test loss: 16.97578 | Test acc: 73.75%\n","\n","\n","Epoch [41/50]\n","Train loss: 41.46566 | Test loss: 16.46226 | Test acc: 73.75%\n","\n","\n","Epoch [42/50]\n","Train loss: 41.15823 | Test loss: 16.08436 | Test acc: 79.06%\n","\n","\n","Epoch [43/50]\n","Train loss: 41.59487 | Test loss: 15.45275 | Test acc: 76.88%\n","\n","\n","Epoch [44/50]\n","Train loss: 41.23135 | Test loss: 15.79036 | Test acc: 76.25%\n","\n","\n","Epoch [45/50]\n","Train loss: 40.69665 | Test loss: 15.53014 | Test acc: 78.44%\n","\n","\n","Epoch [46/50]\n","Train loss: 41.02964 | Test loss: 16.28494 | Test acc: 75.62%\n","\n","\n","Epoch [47/50]\n","Train loss: 40.51142 | Test loss: 16.62496 | Test acc: 74.06%\n","\n","\n","Epoch [48/50]\n","Train loss: 40.07524 | Test loss: 15.59866 | Test acc: 76.88%\n","\n","\n","Epoch [49/50]\n","Train loss: 40.53533 | Test loss: 16.42311 | Test acc: 74.69%\n","\n","\n","Epoch [50/50]\n","Train loss: 39.90383 | Test loss: 15.98744 | Test acc: 74.38%\n","\n"]}],"source":["#model = MLPClassif(30*30, 100, 1)\n","num_epochs = 50\n","learning_rate = 0.081\n","loss_fn = nn.BCEWithLogitsLoss()\n","model = model.cuda()\n","print(next(model.parameters()).is_cuda)\n","model_trained, train_losses, val_losses =train_val_binary_classifier(model, \n","                                                                     train_aug_loader,\n","                                                                     valid_data_loader,\n","                                                                     num_epochs,loss_fn, \n","                                                                     learning_rate, \n","                                                                     verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-26T22:04:41.038365Z","iopub.status.idle":"2022-12-26T22:04:41.039599Z","shell.execute_reply":"2022-12-26T22:04:41.039370Z","shell.execute_reply.started":"2022-12-26T22:04:41.039346Z"},"trusted":true},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, num_channels1=16, num_channels2=32, num_classes=1):\n","        super(CNN, self).__init__()\n","        self.conv_block1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels= num_channels1, kernel_size=5, padding=2),\n","                           nn.ReLU(),\n","                           #nn.BatchNorm2d(num_channels1, affine=False),\n","                           nn.MaxPool2d(kernel_size=2))\n","        \n","        self.conv_block2 = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2),\n","                           nn.ReLU(),\n","                           #nn.BatchNorm2d(num_channels2,affine=False),\n","                           nn.MaxPool2d(kernel_size=2))\n","        self.fc = nn.Linear(32*7*7, num_classes)\n","\n","    def forward(self, x):\n","        x = self.conv_block1(x)\n","        x = self.conv_block2(x)\n","        out = self.fc(x.view(-1, 32*7*7))\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-26T22:04:41.040874Z","iopub.status.idle":"2022-12-26T22:04:41.041701Z","shell.execute_reply":"2022-12-26T22:04:41.041464Z","shell.execute_reply.started":"2022-12-26T22:04:41.041440Z"},"trusted":true},"outputs":[],"source":["cnn_model = CNN()\n","cnn_model = cnn_model.cuda()\n","print(next(cnn_model.parameters()).is_cuda)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-26T22:04:41.043423Z","iopub.status.idle":"2022-12-26T22:04:41.044372Z","shell.execute_reply":"2022-12-26T22:04:41.044135Z","shell.execute_reply.started":"2022-12-26T22:04:41.044108Z"},"trusted":true},"outputs":[],"source":["def training_cnn_classifier(model, train_dataloader, num_epochs,\n","                            loss_fn, learning_rate, verbose=True):\n","\n","    model_tr = copy.deepcopy(model)\n","    model_tr = model_tr.to(device)\n","    \n","    model_tr.train()\n","    optimizer = torch.optim.SGD(model_tr.parameters(), lr=learning_rate)\n","    \n","    # Initialize a list to record the training loss over epochs\n","    loss_all_epochs = []\n","    transform = transforms.Resize(size = (30,30))\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        # Initialize the training loss for the current epoch\n","        loss_current_epoch = 0\n","        \n","        # Iterate over batches using the dataloader\n","        for batch_index, (images, labels) in enumerate(train_dataloader):\n","            images = transform(images).to(device)\n","            labels = labels.to(device)\n","            y_pred = model_tr(images)\n","            l = loss_fn(y_pred, labels)\n","            optimizer.zero_grad()\n","            l.backward()\n","            loss_current_epoch += l.item()\n","            optimizer.step() #update parameters\n","\n","        # At the end of each epoch, record and display the loss over all batches\n","        loss_all_epochs.append(loss_current_epoch)\n","        if verbose:\n","            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n","        \n","    return model_tr, loss_all_epochs\n","\n","def eval_cnn_classifier(model, eval_dataloader):\n","    sigmoid = nn.Sigmoid()\n","    # Set the model in evaluation mode\n","    model.eval() \n","\n","    # In test phase, we don't need to compute gradients (for memory efficiency)\n","    with torch.no_grad():\n","        # initialize the total and correct number of labels to compute the accuracy\n","        correct = 0\n","        total = 0\n","        for images, labels in eval_dataloader:\n","            images = transform_(images).to(device)\n","            labels = labels.to(device)\n","            y_predicted = model(images)\n","            label_predicted = torch.round(sigmoid(y_predicted)).squeeze(1) #y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\n","            total += labels.size(0)\n","            correct += (label_predicted == labels).sum().item()\n","    \n","    accuracy = 100 * correct / total\n","    \n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-26T22:04:41.045749Z","iopub.status.idle":"2022-12-26T22:04:41.046290Z","shell.execute_reply":"2022-12-26T22:04:41.046059Z","shell.execute_reply.started":"2022-12-26T22:04:41.046036Z"},"trusted":true},"outputs":[],"source":["model_cnn = CNN()\n","model_cnn.to(device)\n","num_epochs = 30\n","learning_rate = 0.1\n","loss_fn = nn.BCEWithLogitsLoss()\n","transform_ = transforms.Resize(size = (30,30))\n","model_cnn.train()\n","    \n","    # Define the optimizer\n","optimizer = torch.optim.SGD(model_cnn.parameters(), lr=learning_rate)\n","    \n","    # Initialize a list to record the training loss over epochs\n","loss_all_epochs = []\n","    \n","    # Training loop\n","for epoch in range(num_epochs):\n","        # Initialize the training loss for the current epoch\n","    loss_current_epoch = 0\n","        \n","        # Iterate over batches using the dataloader\n","    for batch_index, (images, labels) in enumerate(train_data_loader):\n","        #print(images.shape)\n","        images = transform_(images).to(device)\n","        labels = labels.to(device)\n","        y_pred = model_cnn(images)\n","        # print(y_pred.shape)\n","        l = loss_fn(y_pred, labels.view(-1, 1))\n","        optimizer.zero_grad()\n","        l.backward()\n","        loss_current_epoch += l.item()\n","        optimizer.step() #update parameters\n","\n","        # At the end of each epoch, record and display the loss over all batches\n","    loss_all_epochs.append(loss_current_epoch)\n","    \n","    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-26T22:04:41.048391Z","iopub.status.idle":"2022-12-26T22:04:41.048875Z","shell.execute_reply":"2022-12-26T22:04:41.048644Z","shell.execute_reply.started":"2022-12-26T22:04:41.048619Z"},"trusted":true},"outputs":[],"source":["eval_cnn_classifier(model_cnn, valid_aug_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
